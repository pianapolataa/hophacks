"""
Seq-to-Seq LSTM prototype that reads its training data from
`synthetic_bp_data.csv` (one row per **hour** per patient).

• Input (per patient)  : the first row’s 14 numerical features + 1-hot sex
                         → 15-dim static vector.
• Output (per patient) : next-8-hour sequence  (SBP, DBP, HR)   –  shape 8×3
                       + scalar drug sensitivity (normalised to 0-1).

The file `synthetic_bp_data.csv` can be generated by the upstream script you
posted; if it is missing we call that generator automatically.

NOT FOR CLINICAL USE — demonstration code only
"""

import os, math, random, joblib, csv
from typing import Dict, List, Tuple
import numpy as np, pandas as pd

import torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

# ─────────────────── CONFIG ──────────────────────────────────────────
DEVICE  = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SEQ_LEN = 8
BATCH_SIZE, EPOCHS, LR = 128, 200, 1e-3
TEACHER_FORCING_RATIO = 0.5
RANDOM_SEED = 42
SENS_WEIGHT = 1.0         # weight of sensitivity term in total loss

CSV_PATH        = "synthetic_bp_data.csv"
MODEL_OUT       = "seq2seq_bp_hr_sens_best.pt"
SCALER_X_PATH   = "scaler_X.joblib"
SCALER_Y_PATH   = "scaler_Yseq.joblib"

# Feature names that actually exist in the CSV
NUMERIC_FEATURES = [
    "avg_sbp","avg_dbp","baseline_hr","age","bmi","diabetes",
    "sodium_intake","exercise_today","dose","current_time",
    "current_sbp","current_dbp","current_hr"
]                               # 13
CATEGORICAL_FEATURES = ["sex"]  # will become 0/1
ALL_INPUTS = NUMERIC_FEATURES + CATEGORICAL_FEATURES   # 14+1

# ─────────────────── REPRODUCIBILITY ────────────────────────────────
def seed_everything(seed: int = RANDOM_SEED):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)
seed_everything()

# ─────────────────── DATA LOADING ───────────────────────────────────
def _generate_csv_if_missing(path: str):
    """Runs the upstream generator only when csv is absent."""
    if os.path.exists(path):
        return
    print("[INFO] CSV not found → generating synthetic dataset …")
    import numpy as np, pandas as pd
    # ---- (the original generator from the user’s snippet) -----------
    def sample_baseline():
        return {
            "avg_sbp": np.random.normal(130, 15),
            "avg_dbp": np.random.normal(80, 10),
            "baseline_hr": np.random.normal(70, 8),
            "age": np.random.randint(30, 80),
            "sex": np.random.choice(["M", "F"]),
            "bmi": np.random.normal(27, 4),
            "diabetes": np.random.choice([0, 1], p=[0.7, 0.3]),
            "sodium_intake": np.random.normal(3000, 800),
            "exercise_today": np.random.exponential(0.5),
        }
    def sample_dose():
        return np.random.choice([0, 2.5, 5, 10, 20, 40],
                                p=[0.3, 0.1, 0.2, 0.25, 0.1, 0.05])
    def compute_sensitivity(baseline):
        sens = 1.0
        if baseline["age"] > 60: sens *= 0.85
        if baseline["bmi"] > 30: sens *= 0.9
        if baseline["diabetes"] == 1: sens *= 0.8
        if baseline["sodium_intake"] > 4000: sens *= 0.9
        if baseline["exercise_today"] > 1: sens *= 1.05
        sens = np.clip(sens * np.random.normal(1, 0.1), 0.5, 1.5)
        return sens
    def circadian_factor(hour):
        bp_factor = 5 * np.sin(2 * np.pi * (hour - 9) / 24)
        hr_factor = 3 * np.sin(2 * np.pi * (hour - 15) / 24)
        return bp_factor, hr_factor
    def drug_effect(dose, sensitivity, t):
        if dose == 0:
            return 0
        peak = -0.6 * dose * sensitivity
        return peak * np.exp(-0.3 * t)
    def generate_patient(pid):
        baseline = sample_baseline()
        dose = sample_dose()
        sens = compute_sensitivity(baseline)
        dose_time = np.random.randint(0, 24)
        circ_bp0, circ_hr0 = circadian_factor(dose_time)
        current_sbp = baseline["avg_sbp"] + circ_bp0 + np.random.normal(0, 2)
        current_dbp = baseline["avg_dbp"] + 0.6 * circ_bp0 + np.random.normal(0, 1.5)
        current_hr  = baseline["baseline_hr"] + circ_hr0  + np.random.normal(0, 1)

        recs=[]
        for t in range(8):
            hour_of_day = (dose_time + t) % 24
            circ_bp, circ_hr = circadian_factor(hour_of_day)
            drug_d = drug_effect(dose,sens,t)
            sbp = baseline["avg_sbp"] + circ_bp + drug_d + np.random.normal(0,2)
            dbp = baseline["avg_dbp"] + 0.6*circ_bp + 0.5*drug_d + np.random.normal(0,1.5)
            hr  = baseline["baseline_hr"] + circ_hr - 0.05*drug_d + np.random.normal(0,1)
            recs.append({
                "patient_id":pid, **baseline, "dose":dose, "current_time":dose_time,
                "current_sbp":current_sbp,"current_dbp":current_dbp,"current_hr":current_hr,
                "sensitivity":sens, "sbp":sbp,"dbp":dbp,"hr":hr,"t_postdose":t})
        return recs
    def generate_dataset(n=20000):
        allrec=[]
        for pid in range(n): allrec.extend(generate_patient(pid))
        return pd.DataFrame(allrec)
    df = generate_dataset()
    df.to_csv(path,index=False)
    print("[INFO] synthetic_bp_data.csv generated — shape:",df.shape)

class BPSeqDataset(Dataset):
    """
    Creates per-patient samples from the CSV:
      X  : (num_features,)  – static vector (first row of patient)
      y_seq : (8,3)         – SBP, DBP, HR over next 8 h
      y_sens: (1,)          – sensitivity mapped to 0-1
    Scalers are fitted on the training split only.
    """
    def __init__(self, df: pd.DataFrame,
                 scaler_X: StandardScaler = None,
                 scaler_y_seq: StandardScaler = None,
                 fit: bool = False):
        # ---------------- aggregate per patient -------------------
        X_rows, Yseq, Ys = [], [], []
        for pid, g in df.groupby("patient_id"):
            g = g.sort_values("t_postdose")
            first = g.iloc[0]

            # feature vector
            feats = [first[col] for col in NUMERIC_FEATURES]
            feats.append(1.0 if first["sex"]=="F" else 0.0)   # append sex flag
            X_rows.append(feats)

            # 8×3 target sequence
            ymat = g[["sbp","dbp","hr"]].values.astype(np.float32)
            if len(ymat)!=SEQ_LEN:           # safety (shouldn’t happen)
                continue
            Yseq.append(ymat)

            # sensitivity normalised to 0-1   (range original 0.5-1.5)
            sens_norm = np.clip((first["sensitivity"] - 0.5)/1.0 ,0.0,1.0)
            Ys.append([sens_norm])

        self.X_raw     = np.array(X_rows ,dtype=np.float32)
        self.y_seq_raw = np.array(Yseq   ,dtype=np.float32)
        self.y_sens    = np.array(Ys     ,dtype=np.float32)

        # ---------------- scaling ---------------------------------
        if fit:
            scaler_X = StandardScaler().fit(self.X_raw)
            scaler_y_seq = StandardScaler().fit(self.y_seq_raw.reshape(len(self.y_seq_raw), -1))
        self.scaler_X, self.scaler_y_seq = scaler_X, scaler_y_seq

        self.X   = self.scaler_X.transform(self.X_raw).astype(np.float32)
        flat_seq = self.y_seq_raw.reshape(len(self.y_seq_raw), -1)
        self.y_seq = self.scaler_y_seq.transform(flat_seq).reshape(len(self.y_seq_raw), SEQ_LEN, 3)

    def __len__(self): return len(self.X)
    def __getitem__(self,idx): return self.X[idx], self.y_seq[idx], self.y_sens[idx]

# ─────────────────── MODEL ──────────────────────────────────────────
class Encoder(nn.Module):
    def __init__(self, inp, hid, lat):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(inp,hid), nn.ReLU(),
                                 nn.Linear(hid,lat), nn.ReLU())
    def forward(self,x): return self.net(x)

class Decoder(nn.Module):
    def __init__(self, inp, hid, outp, n_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(inp,hid,n_layers,batch_first=True)
        self.fc   = nn.Linear(hid,outp)
    def forward(self,x,h): o,(hn,cn)=self.lstm(x,h); return self.fc(o),(hn,cn)

class Seq2SeqBP(nn.Module):
    def __init__(self, in_dim, lat=64, enc_h=128, dec_in=3,
                 dec_h=128, out_dim=3, n_layers=1):
        super().__init__()
        self.encoder = Encoder(in_dim, enc_h, lat)
        self.decoder = Decoder(dec_in, dec_h, out_dim, n_layers)
        self.h0_proj = nn.Linear(lat, dec_h)
        self.c0_proj = nn.Linear(lat, dec_h)
        self.sens_head = nn.Sequential(
            nn.Linear(lat, max(8, lat//2)), nn.ReLU(),
            nn.Linear(max(8, lat//2), 1), nn.Sigmoid())

    def forward(self,src,dec_init,trg_seq=None,tf_ratio=0.5):
        lat   = self.encoder(src)
        sens  = self.sens_head(lat)
        h0    = torch.tanh(self.h0_proj(lat)).unsqueeze(0)
        c0    = torch.tanh(self.c0_proj(lat)).unsqueeze(0)
        hidden= (h0,c0)

        out_seq=[]
        dec_in = dec_init
        for t in range(SEQ_LEN):
            pred,hidden = self.decoder(dec_in,hidden)   # (B,1,3)
            out_seq.append(pred)
            use_teacher = trg_seq is not None and random.random()<tf_ratio
            dec_in = trg_seq[:,t:t+1] if use_teacher else pred.detach()
        out_seq = torch.cat(out_seq, dim=1)             # (B,8,3)
        return out_seq, sens

# ─────────────────── TRAIN / EVAL ────────────────────────────────
def train_epoch(model,loader,opt,crit_seq,crit_sens):
    model.train(); total=0
    sbp_i=NUMERIC_FEATURES.index("current_sbp")
    dbp_i=NUMERIC_FEATURES.index("current_dbp")
    hr_i =NUMERIC_FEATURES.index("current_hr")
    for X,y_seq,y_sens in loader:
        X,y_seq,y_sens = X.to(DEVICE),y_seq.to(DEVICE),y_sens.to(DEVICE)
        dec_init = torch.cat([X[:,sbp_i:sbp_i+1],
                              X[:,dbp_i:dbp_i+1],
                              X[:,hr_i :hr_i +1]],1).unsqueeze(1)
        opt.zero_grad()
        p_seq,p_sens = model(X,dec_init,trg_seq=y_seq,tf_ratio=TEACHER_FORCING_RATIO)
        loss = crit_seq(p_seq,y_seq) + SENS_WEIGHT*crit_sens(p_sens,y_sens)
        loss.backward(); opt.step()
        total += loss.item()*X.size(0)
    return total/len(loader.dataset)

@torch.no_grad()
def eval_epoch(model,loader,crit_seq,crit_sens):
    model.eval(); total=0
    sbp_i=NUMERIC_FEATURES.index("current_sbp")
    dbp_i=NUMERIC_FEATURES.index("current_dbp")
    hr_i =NUMERIC_FEATURES.index("current_hr")
    for X,y_seq,y_sens in loader:
        X,y_seq,y_sens = X.to(DEVICE),y_seq.to(DEVICE),y_sens.to(DEVICE)
        dec_init = torch.cat([X[:,sbp_i:sbp_i+1],
                              X[:,dbp_i:dbp_i+1],
                              X[:,hr_i :hr_i +1]],1).unsqueeze(1)
        p_seq,p_sens = model(X,dec_init,trg_seq=None,tf_ratio=0.0)
        total += (crit_seq(p_seq,y_seq)+SENS_WEIGHT*crit_sens(p_sens,y_sens)).item()*X.size(0)
    return total/len(loader.dataset)

# ─────────────────── TRAIN DRIVER ────────────────────────────────
def fit_from_csv(csv_path=CSV_PATH, epochs=EPOCHS, batch=BATCH_SIZE, lr=LR):
    _generate_csv_if_missing(csv_path)
    full_df = pd.read_csv(csv_path)

    # split by patient_id
    pids = full_df["patient_id"].unique()
    np.random.shuffle(pids)
    split = int(0.8*len(pids))
    train_df = full_df[full_df["patient_id"].isin(pids[:split])].reset_index(drop=True)
    val_df   = full_df[full_df["patient_id"].isin(pids[split:])].reset_index(drop=True)

    train_ds = BPSeqDataset(train_df, fit=True)
    val_ds   = BPSeqDataset(val_df,
                            scaler_X=train_ds.scaler_X,
                            scaler_y_seq=train_ds.scaler_y_seq,
                            fit=False)
    joblib.dump(train_ds.scaler_X , SCALER_X_PATH)
    joblib.dump(train_ds.scaler_y_seq , SCALER_Y_PATH)
    print("[INFO] scalers saved.")

    train_loader = DataLoader(train_ds,batch,shuffle=True)
    val_loader   = DataLoader(val_ds  ,batch,shuffle=False)

    model = Seq2SeqBP(in_dim=train_ds.X.shape[1]).to(DEVICE)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    crit_seq, crit_sens = nn.MSELoss(), nn.MSELoss()

    best=np.inf
    for ep in range(1,epochs+1):
        tr=train_epoch(model,train_loader,opt,crit_seq,crit_sens)
        va=eval_epoch(model,val_loader ,crit_seq,crit_sens)
        print(f"Epoch {ep:3d}/{epochs}  Train {tr:.4f}  Val {va:.4f}")
        if va<best:
            best=va
            torch.save({"model_state":model.state_dict(),
                        "scaler_X":SCALER_X_PATH,"scaler_Y":SCALER_Y_PATH}, MODEL_OUT)
            print("   ↳ best model saved")
    print("Best val loss:",best)
    return model,train_ds.scaler_X,train_ds.scaler_y_seq

# ─────────────────── PREDICTION UTILITY ───────────────────────────
@torch.no_grad()
def load_model_artifacts(model_path=MODEL_OUT):
    ckpt=torch.load(model_path,map_location=DEVICE)
    scX=joblib.load(ckpt["scaler_X"]); scY=joblib.load(ckpt["scaler_Y"])
    model=Seq2SeqBP(in_dim=len(scX.mean_)).to(DEVICE)
    model.load_state_dict(ckpt["model_state"]); model.eval()
    return model, scX, scY

@torch.no_grad()
def predict_patient(model,scX,scY,patient_rows:pd.DataFrame)->Tuple[pd.DataFrame,float]:
    g=patient_rows.sort_values("t_postdose")
    first=g.iloc[0]
    feats=[first[col] for col in NUMERIC_FEATURES]+[1.0 if first["sex"]=="F" else 0.0]
    X=torch.tensor(scX.transform(np.array(feats,dtype=np.float32).reshape(1,-1)),
                   device=DEVICE)
    sbp_i=NUMERIC_FEATURES.index("current_sbp")
    dbp_i=NUMERIC_FEATURES.index("current_dbp")
    hr_i =NUMERIC_FEATURES.index("current_hr")
    dec_init=torch.cat([X[:,sbp_i:sbp_i+1],X[:,dbp_i:dbp_i+1],X[:,hr_i:hr_i+1]],1).unsqueeze(1)

    seq_scaled, sens = model(X,dec_init,trg_seq=None,tf_ratio=0.0)
    seq = scY.inverse_transform(seq_scaled.cpu().numpy().reshape(1,-1)).reshape(SEQ_LEN,3)
    sens_val=float(sens.item())
    base_hour=int(first["current_time"])
    df_out=pd.DataFrame({
        "hour_offset":np.arange(1,SEQ_LEN+1),
        "target_hour":[(base_hour+i)%24 for i in range(1,SEQ_LEN+1)],
        "sbp_pred":seq[:,0],"dbp_pred":seq[:,1],"hr_pred":seq[:,2]
    })
    return df_out, sens_val

# ─────────────────── MAIN DEMO ─────────────────────────────────────
if __name__=="__main__":
    model,scX,scY = fit_from_csv()

    print("\n◎ Loading best checkpoint for demo inference …")
    model,scX,scY = load_model_artifacts()

    # pick one random patient from CSV to demo
    csv_df = pd.read_csv(CSV_PATH)
    pid_demo = csv_df["patient_id"].sample(1).item()
    demo_rows = csv_df[csv_df["patient_id"]==pid_demo]

    preds, sens = predict_patient(model,scX,scY,demo_rows)
    print("\nPredictions for patient",pid_demo)
    print(preds.to_string(index=False))
    print(f"Sensitivity (0-1): {sens:.3f}")
    print("\nArtifacts:",MODEL_OUT,SCALER_X_PATH,SCALER_Y_PATH)